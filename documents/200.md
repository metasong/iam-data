# AI Backpropagation
---
author: Jason Song <metaseed@gmail.com>
version: 1.0.0
tag: []
subPage: []
enable: [toc]

---

http://colah.github.io/posts/2015-08-Backprop/
https://cs231n.github.io/optimization-2/

## Computational Graphs
![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T184704495Z-image.png)
![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T184748118Z-image.png)

## Derivatives on Computational Graphs
![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T184941573Z-image.png)

![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T184952213Z-image.png)


d(f(g(x)))/dx = df/dg * dg/dx

de = d(c * d) = d * de/dc + c * de/d(d) 
> '+'  together

![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T185304360Z-image.png)


## Factoring Paths
![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T185051050Z-image.png)

![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T185610905Z-image.png)

![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T185811748Z-image.png)

![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T191023466Z-image.png)

Forward-mode differentiation tracks how one input affects every node. Reverse-mode differentiation tracks how every node affects one output. 

## Computational Victories
![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T184952213Z-image.png)
### forward-mode
![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T191447348Z-image.png)

### backward-mode
![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T191520795Z-image.png)

de/da = de/dc * dc/da = 2 * 1 = 2

de/db = de/dc * dc/db + de/dd * dd/db = 2 * 1  + 3 * 1 = 5

When training neural networks, we think of the cost (a value describing how bad a neural network performs) as a function of the parameters (numbers describing how the network behaves). We want to calculate the derivatives of the cost with respect to all the parameters, for use in gradient descent. Now, thereâ€™s often millions, or even tens of millions of parameters in a neural network. So, reverse-mode differentiation, called backpropagation in the context of neural networks, gives us a massive speed up!

Where the reverse-mode gives the derivatives of one output with respect to all inputs, the forward-mode gives us the derivatives of all outputs with respect to one input. If one has a function with lots of outputs, forward-mode differentiation can be much, much, much faster.

## 
http://neuralnetworksanddeeplearning.com/chap2.html#the_hadamard_product_$s_\odot_t$
Hadamard product or Schur product. 
![](https://raw.githubusercontent.com/metasong/iam-data/master/documents/200/image/20230602T193116120Z-image.png)